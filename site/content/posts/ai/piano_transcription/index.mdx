---
title: "AI 프로젝트 - 피아노 음표 감지"
category: "인공 지능"
author: kuper0201
tags: ['#AI', '#projects']
date: 2023-09-11
slug: piano-transcription
thumbnail: cover.jpg
draft: true
---

## 서론

피아노 연주를 악보의 형태로 변환하는 작업을 "채보(Transcription)"라 합니다.

기존의 채보는 인간이 직접 연주를 청취하여 악보로 변환하는 방식으로 이루어 졌습니다.

이는 청취하는 사람에 따라 결과의 정확도가 달라질 뿐 아니라 시간 또한 무척 오래 걸리는 방식이었습니다.

채보 작업을 AI 모델의 학습을 통해 자동화 시킨다면 비교적 높은 정확도와 빠른 시간 내에 작업을 완료 할 수 있습니다.

따라서 본 프로젝트에서는 피아노 연주(WAV, MP3) 등을 입력받아 컴퓨터가 연주 할 수 있는 형태인 MIDI 형태로 변환할 수 있는 채보 모델을 학습시키고자 합니다.

## 데이터셋 확보

AI 모델을 학습하기 위한 첫 단계는 데이터를 확보하는 일입니다.

데이터셋을 선정하기에 앞서 프로젝트 진행에 필요한 조건을 선정하였습니다.

1. 피아노 채보 모델을 학습할 것 이므로, 다른 악기의 오디오는 포함되지 않음
2. 지도학습에는 입력 데이터와 정답 데이터가 필요하므로, 오디오와 악보 데이터가 존재

상기한 조건을 만족하는 데이터셋을 조사해 본 결과 아래와 같이 3종류의 데이터셋을 찾을 수 있었습니다.

1. YAMAHA MIDI 데이터셋
2. MAPS 데이터셋
3. Magenta MAESTRO 데이터셋

YAMAHA MIDI 데이터셋은 피아노 연주를 MIDI 파일의 형태로 제공하여 해당 MIDI 파일을 모두 오디오 파일로 변환하는 작업이 필요합니다.

MAPS 데이터셋은 단일 음표, 다중 음표 등 다양한 데이터들로 이루어져 있습니다.
하지만 각 데이터의 길이가 짧아 음악의 긴 패턴을 학습하기에는 부족해 보였습니다.

마지막으로 Magenta MAESTRO 데이터셋은 MIDI 파일과 WAV 파일을 모두 제공해 주었습니다.
또한, 전체 음악에 대한 데이터이므로 음악의 긴 패턴을 학습하기에도 적합해 보였습니다.

따라서 본 프로젝트에서는 Magenta프로젝트의 MAESTRO 데이터셋을 이용하기로 했습니다.

## 모델 선택

음악과 같은 시계열 데이터는 RNN(Recurrent Neural Network)을 이용한 방식으로 처리하는 것이 일반적입니다.

하지만 RNN은 "기울기 소실" 또는 "기울기 폭발" 문제로 인해 장기 의존성을 효과적으로 학습하기 어렵다는 문제를 가지고 있습니다.

쉽게 말해 시퀀스의 길이가 길어질수록 학습해야 할 기울기가 희석되어 학습이 불안정해지거나 어려워진다는 것 입니다.

이러한 문제를 개선하기 위해 제안된 것이 LSTM(Long Short Term Memory)입니다.

LSTM은 RNN의 변형으로, 상기한 RNN의 문제점들을 완화하여 장기 의존성을 효과적으로 학습 할 수 있습니다.

따라서 해당 프로젝트에서는 LSTM(Long Short Term Memory)를 이용한 방식으로 모델을 구축하기로 계획하였습니다.

## 전처리 방식 분석

머신러닝 모델을 훈련하기 전에 데이터를 분석하고 처리하는 과정은 매우 중요합니다.

데이터를 분석하기에 앞서, 우선 소리의 구성에 대한 이해가 필요합니다.

소리는 진동으로 인해 발생하는 파형으로 표현되며, 이 파형은 여러 다른 주파수의 파장들이 서로 어우러져 하나의 복합 신호를 형성합니다.

이론적으로는 모델이 이러한 복합 신호를 직접 처리할 수 있어야 하지만, 실제 모델의 구현 결과 다양한 주파수, 진폭, 위상 등의 파형 구성이 다양하기 때문에 복합 신호의 모든 패턴을 학습하기는 어려웠습니다.

이러한 문제를 해결하기 위해 원본 복합 신호를 구성하는 파형을 추출하고 이를 모델에 입력 데이터로 사용하는 전처리 과정의 필요성을 느끼게 되었고, 복합 신호에서 각각의 구성 파형을 추출할 수 있는 기법을 조사해 보았습니다.

### Fourier Transform

원본 데이터에서 파형을 추출하는 방법 중 하나로 푸리에 변환(Fourier Transform)이 존재합니다.

푸리에 변환은 복합 신호를 구성하는 파형들을 추출 할 수 있는 변환 기법입니다.

하지만 푸리에 변환을 사용하면 원본 신호에서 구성 파형을 추출할 수 있지만, 각 파형의 시간 정보를 잃어버린다는 문제가 존재합니다.

아래 이미지는 "젓가락 행진곡" 오디오를 푸리에 변환한 이미지입니다.

![Fourier Transform](images/FourierTransform.png)

이미지를 확인해 보면 상기하였듯 복합 신호에서 구성 주파수들을 추출하였지만 시간 정보는 잃어버린 것을 볼 수 있습니다.

피아노 채보를 위해서는 시간의 흐름에 따라 연주되는 파형을 음표의 형태로 변환해야 하므로 시간 정보를 보존하는 것은 필수적이므로 푸리에 변환은 해당 프로젝트에 적합하지 않다고 결론 내렸습니다.

### STFT

푸리에 변환이 시간 정보를 잃어버린다는 문제를 고려하기 위한 기법이 STFT입니다.

STFT(Short Time Fourier Transform)는 전체 신호를 작은 시간 단위(Frame)로 나누어서 각 시간 단위에 대한 푸리에 변환을 수행합니다.

따라서 주파수 정보와 시간 정보를 동시에 보존 할 수 있습니다.

아래의 이미지는 "젓가락 행진곡"을 STFT를 이용해 스펙트로그램으로 변환한 이미지입니다.

![STFT](images/STFT.png)

상기하였듯 시간 정보를 유지하며 시간에 따른 주파수를 올바르게 추출하는 것을 볼 수 있습니다.

### CQT

시간 정보를 유지하며 구성 주파수를 추출하는 또 다른 기법으로 CQT(Constant-Q Transform)이 존재합니다.

CQT(Constant-Q Transform)는 주파수 대역을 로그 스케일로 나누어 변환하는 방법입니다.

이 방법은 주파수 대역을 고정된 비율(로그 스케일)로 분할하여 각 시간 단위에 대한 주파수 성분을 추출하여 주파수 정보와 시간 정보를 모두 보존할 수 있습니다.

아래의 이미지는 "젓가락 행진곡"을 CQT를 이용해 스펙트로그램으로 변환한 이미지입니다.

![CQT](images/CQT.png)

각 이미지를 비교하면 알 수 있듯 CQT가 STFT에 비해 고조파(Harmonic)에 비교적 강한 특성을 보였기 때문에 해당 프로젝트에서는 CQT를 이용한 주파수 추출 방식을 채택하였습니다.

## 데이터 전처리 수행

전처리 방식을 선택하였으므로 데이터셋의 WAV파일과 MIDI 데이터를 Numpy 배열로 변환하여야 합니다.

librosa 라이브러리는 오디오 신호 처리를 위한 라이브러리로, 푸리에 변환, STFT, CQT 등의 작업을 비교적 쉽게 수행할 수 있게 해 줍니다.

CQT 변환을 위해서는 시간, 주파수 단위 각각의 해상도를 지정해야 합니다.

### 시간 해상도

CQT에서 시간 해상도는 SR(Sample Rate)와 Hop Length에 의해 결정됩니다.

일반적으로 사용되는 SR은 44100이지만, 연산에 필요한 비용을 고려해 16000으로 WAV 파일을 다운샘플링 하였습니다.

또한 HOP_LENGTH를 160으로 설정하여 0.01초 단위의 시간 해상도를 가지게 하였습니다.

### 주파수 해상도

CQT에서 주파수 해상도는 bins에 의해 결정됩니다.

하나의 옥타브는 총 12개의 음으로 구성되므로, 하나의 음을 3개의 bins를 이용해 표현하기 위해 bins_per_octave를 36으로 설정하였습니다.

또한 피아노는 총 88개의 건반을 가지므로 264(3 * 88)의 n_bins를 설정하여 주파수 해상도를 설정하였습니다.

### 타겟 데이터 전처리

모델에 공급하기 위한 입력 데이터의 전처리를 마쳤으므로, 모델이 예측할 타겟 데이터를 생성해야 합니다.

피아노 건반의 개수가 88개이므로 데이터셋의 MIDI파일을 원-핫 인코딩을 수행하여 (오디오 시간, 88) 형태의 Numpy 배열로 변환하여 저장했습니다.

(오디오 시간, 88) 형태의 데이터는 각 시간 스텝에 해당하는 피아노 건반의 상태를 나타냅니다.

이는 88개의 피아노 건반 중 해당 시간 스텝에서 연주되고 있는 피아노 건반을 의미합니다.

### 배치 분할

LSTM 레이어는 (Batch, Timestep, Feature) 모양의 텐서를 입력으로 받습니다.

한번에 모든 데이터를 모델에 공급하는 것은 불가능하기 때문에 데이터를 100개의 고정된 타임스텝으로 분할하였습니다.

따라서 입력 데이터와 타겟 데이터는 최종적으로 (오디오 시간 / 100, 100, 264)의 형식으로 나누어집니다.

이는 0.01초 단위의 데이터를 100개 단위로 묶어 모델에 공급한다는 의미로 한 번에 1초의 데이터를 공급하겠다는 의미입니다.

