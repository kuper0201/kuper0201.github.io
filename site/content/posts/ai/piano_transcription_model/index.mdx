---
title: "피아노 음악 채보 - 2.모델 구현"
category: "인공 지능"
author: kuper0201
tags: ['#AI', '#projects']
date: 2023-09-12
slug: piano-transcription-model
thumbnail: cover.jpg
draft: false
---

## 바로가기

### &nbsp; &nbsp; &nbsp; &nbsp;[피아노 음악 채보 - 1.전처리](https://jeong-jun.su/piano-transcription-preproc)
### &nbsp; &nbsp; &nbsp; &nbsp;[피아노 음악 채보 - 2.모델 구현(현재 글)](#서론)
### &nbsp; &nbsp; &nbsp; &nbsp;[피아노 음악 채보 - 3.후처리 및 성능 평가](https://jeong-jun.su/piano-transcription-postproc)

---

## 서론

이전 글에서는 피아노 채보 모델 학습을 위한 데이터셋을 확보하고 해당 데이터셋의 전처리를 수행하였습니다.

이번 글에서는 전처리된 데이터를 공급하여 피아노 음악을 채보 할 수 있는 모델을 구성하고 학습을 시켜보겠습니다.

## 모델 아키텍처 선택

머신 러닝에는 Transformer, RNN, DNN, CNN 등 무수히 많은 이키텍처들이 존재합니다.

가장 먼저 고려한 아키텍처는 CNN이었습니다. 스펙트로그램은 시간 축과 주파수 축을 가진 2차원 데이터이므로 같은 2차원 데이터인 이미지를 효과적으로 처리 할 수 있는 아키텍처인 CNN을 이용하는 것이 적합해 보였습니다.

// 왜 CNN을 사용하지 않았는가?

다음으로는 RNN 아키텍처를 고려하였습니다. RNN 아키텍처는 시계열 데이터의 처리에 강하다는 특징이 있기 때문입니다. 하지만 RNN 아키텍처에는 "기울기 소실", "기울기 폭발" 문제가 존재하여 장기 의존성을 효과적으로 학습하기 어렵다는 문제점을 가지고 있습니다. 쉽게 말해, 시퀀스(입력 데이터)의 길이가 길어질수록 학습해야 할 기울기가 희석되어 학습이 어려워지거나 불안정해진다는 것 입니다.

이러한 문제를 개선하기 위해 제안된 것이 LSTM(Long Short Term Memory)입니다. LSTM은 RNN의 변형으로, 상기한 RNN의 문제점들을 완화하여 장기 의존성을 효과적으로 학습 할 수 있습니다.

따라서 해당 프로젝트에서는 LSTM(Long Short Term Memory)을 이용한 방식으로 모델을 구축하기로 계획하였습니다.

## 모델 구성

모델의 아키텍처를 선택하였으므로 텐서플로우를 이용하여 실제 모델을 구성해 보겠습니다.

모델에서 음의 Onset(시작점)과 Length(길이)를 모두 예측할 수 있기를 원하기 때문에 두 개의 모델을 따로 학습하였습니다.

Onset 모델은 음의 시작점을 예측하는 모델이고, Length 모델은 음의 길이를 예측하는 모델입니다.

두 모델의 구조는 동일하며 입력으로 Onset 데이터를 입력으로 받는지 Length 데이터를 입력으로 받는지에 대한 차이만이 존재합니다.

추후 후처리 과정을 통해 두 모델의 출력을 하나로 병합하여 MIDI 파일로 변환할 것입니다.

모델 구성의 세부사항은 아래 코드에 주석으로 설명해 두었습니다.

<details markdown="1">
<summary>코드 보기(Python)</summary>

```javascript
import glob
import numpy as np
from keras import Model
from tensorflow import keras
from keras.layers import Dense, Input, Activation, Bidirectional, LSTM, TimeDistributed
from keras.callbacks import ModelCheckpoint, EarlyStopping
import tensorflow.python.keras.optimizers

import warnings
warnings.filterwarnings("ignore")

import tensorflow.python.keras.mixed_precision.policy as mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

train_x, train_onset = [], []
valid_x, valid_onset = [], []

model = None

# 데이터셋 제너레이터 함수
def dataSetGenerator(x_path, onset_path):
    for a, b in zip(x_path, onset_path):
        X, ONSET = [], []
        X.append(np.load(a))
        ONSET.append(np.load(b))

        X = np.concatenate(X, axis=0)
        ONSET = np.concatenate(ONSET, axis=0)

        X = X / np.max(X)
        
        # 배치 간의 연결 끊기 위한 함수
        model.reset_states()
        
        for x, onset in zip(X, ONSET):
            yield (x, onset)

# 모델 생성 함수
def buildModel():
    # 입력 레이어
    input_layer = Input(batch_input_shape=(10, 100, 264), name='onset_input')

    # LSTM 레이어
    onset_lstm = Bidirectional(LSTM(128, activation='tanh', return_sequences=True, stateful=True, name='onset_lstm'))(input_layer)

    # 출력 레이어
    onset_out = TimeDistributed(Dense(88, activation='sigmoid', kernel_initializer='he_normal', name='onset_output'))(onset_lstm)

    model = keras.Model(inputs=input_layer, outputs=onset_out)
    return model

def train(trainX, trainOnset, validX, validOnset):
    for x_name, onset_name in zip(glob.glob(trainX), glob.glob(trainOnset)):
        train_x.append(x_name)
        train_onset.append(onset_name)

    for x_name, onset_name in zip(glob.glob(validX), glob.glob(validOnset)):
        valid_x.append(x_name)
        valid_onset.append(onset_name)

    input_signature = (tensorflow.float32, tensorflow.int8)
    in_out_shape = ([100, 264], [100, 88])

    global model
    model = buildModel()
    trainSet = tensorflow.data.Dataset.from_generator(dataSetGenerator, input_signature, in_out_shape, args=[train_x, train_onset])
    validSet = tensorflow.data.Dataset.from_generator(dataSetGenerator, input_signature, in_out_shape, args=[valid_x, valid_onset])
    
    # 배치 크기를 10으로, 남는 배치는 버림
    trainSet = trainSet.batch(10, drop_remainder=True).prefetch(tensorflow.data.experimental.AUTOTUNE)
    validSet = validSet.batch(10, drop_remainder=True).prefetch(tensorflow.data.experimental.AUTOTUNE)
    
    # 체크포인트, 조기 종료 콜백 설정
    checkpoint = ModelCheckpoint('onset_detector.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')
    early_stop = EarlyStopping(patience=5, monitor='val_loss', verbose=1, mode='auto')

    opt = tensorflow.optimizers.Adam(learning_rate=0.0005)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    
    # 순차적인 패턴 학습해야 하므로, shuffle을 하지 않음
    model.fit(trainSet, validation_data=validSet, epochs=1000, shuffle=False, callbacks=[checkpoint, early_stop])

if __name__ == '__main__':
    train('../PreProc/trainX/*.npy', '../PreProc/trainONSET/*.npy', '../PreProc/validX/*.npy', '../PreProc/validONSET/*.npy')
```
</details>

## 모델 학습

모델의 구성을 완료하고, 실제로 데이터를 입력하여 모델을 학습시켰습니다.

Nvidia의 GTX1080 8GB 장비로 약 이틀의 시간이 소요되었습니다.

## 마무리

모델의 구성과 학습을 진행하였습니다.

다음으로는 각 모델의 출력을 하나의 MIDI 파일로 합치는 후처리 과정과 오디오 데이터의 예측 및 모델의 성능 평가에 대한 글을 작성해 보겠습니다.